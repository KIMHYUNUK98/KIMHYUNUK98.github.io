{
    "componentChunkName": "component---src-templates-blog-post-tsx",
    "path": "/blog/deeplearning5/",
    "result": {"data":{"markdownRemark":{"html":"<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.41717791411043%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5oTWH/xAAYEAEBAAMAAAAAAAAAAAAAAAABEAARE//aAAgBAQABBQKcxdYT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAEBEgIZH/2gAIAQEABj8CDvYf/8QAHBABAAIBBQAAAAAAAAAAAAAAAQARECExQVGR/9oACAEBAAE/IUb59gsXE2YdzNZfeP/aAAwDAQACAAMAAAAQQM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAQACAwAAAAAAAAAAAAABEQAxUSFBYf/aAAgBAQABPxAilC99BKE7yq6ViDDQ9+eMEHPiYJxv/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/5_1.jpg\"\n        srcset=\"/static/d41d8cd98f00b204e9800998ecf8427e/d2f63/5_1.jpg 163w,\n/static/d41d8cd98f00b204e9800998ecf8427e/c989d/5_1.jpg 325w,\n/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/5_1.jpg 650w,\n/static/d41d8cd98f00b204e9800998ecf8427e/7c09c/5_1.jpg 975w,\n/static/d41d8cd98f00b204e9800998ecf8427e/fa90f/5_1.jpg 1236w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\r\n<span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 72.39263803680981%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAIBAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHblFHKw//EABoQAQACAwEAAAAAAAAAAAAAAAEAAgMREjH/2gAIAQEAAQUCfdzidjC5aOeo/wD/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAbEAACAQUAAAAAAAAAAAAAAAAAEAERITFxkf/aAAgBAQAGPwJZnq0Uuf/EABkQAQEBAQEBAAAAAAAAAAAAAAERACExYf/aAAgBAQABPyGj1hfNKr64dpeM1+D1MiRZv//aAAwDAQACAAMAAAAQkA//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAeEAEBAAIABwAAAAAAAAAAAAABEQAhMUFRYXGR0f/aAAgBAQABPxC5tHfFznqDvi+mqyiYaiRFQ+4sy2dDb7xupYoHTzn/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/b491ddffbb96accddacf539f76e0cd57/6aca1/5_2.jpg\"\n        srcset=\"/static/b491ddffbb96accddacf539f76e0cd57/d2f63/5_2.jpg 163w,\n/static/b491ddffbb96accddacf539f76e0cd57/c989d/5_2.jpg 325w,\n/static/b491ddffbb96accddacf539f76e0cd57/6aca1/5_2.jpg 650w,\n/static/b491ddffbb96accddacf539f76e0cd57/7c09c/5_2.jpg 975w,\n/static/b491ddffbb96accddacf539f76e0cd57/481c6/5_2.jpg 1245w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Gradient Descent</h3>\n<p><strong>미분을 통해서 Optimize</strong>하게 된다. 하나의 스칼라 값을 W에 대해 미분을 하게 된다. W의 값을 일렬로 세워서 일차원 벡터인 것으로 생각하자.</p>\n<p>scalar 값을 나열하면 다음과 같은 식을 얻을 수 있다. (가장 오른쪽 식) 이러한 벡터를 Gradient 벡터라고 한다.</p>\n<p>각각 Element들이 해당 Weight로 미분이 된것이 Gradient 벡터라고 한다. <strong>수학적으로 Error surface의 error가 급격하게 변화는 방향</strong>이라고 볼 수 있다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.41717791411043%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5oTWH/xAAYEAEBAAMAAAAAAAAAAAAAAAABEAARE//aAAgBAQABBQKcxdYT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAEBEgIZH/2gAIAQEABj8CDvYf/8QAHBABAAIBBQAAAAAAAAAAAAAAAQARECExQVGR/9oACAEBAAE/IUb59gsXE2YdzNZfeP/aAAwDAQACAAMAAAAQQM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAQACAwAAAAAAAAAAAAABEQAxUSFBYf/aAAgBAQABPxAilC99BKE7yq6ViDDQ9+eMEHPiYJxv/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/5_3.jpg\"\n        srcset=\"/static/d41d8cd98f00b204e9800998ecf8427e/d2f63/5_3.jpg 163w,\n/static/d41d8cd98f00b204e9800998ecf8427e/c989d/5_3.jpg 325w,\n/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/5_3.jpg 650w,\n/static/d41d8cd98f00b204e9800998ecf8427e/7c09c/5_3.jpg 975w,\n/static/d41d8cd98f00b204e9800998ecf8427e/68a11/5_3.jpg 1246w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>W에 Gradient를 마이너스 한다는 것은 반대 방향으로 간다는 것을 의미한다. 감소하는 방향으로 이동하게 된다. <strong>현재 Weight에서 Gradient를 빼주는 것이 Gradient Minimize pattern이라고 한다.</strong> 미분을 해서 Gradient Vector를 찾았지만 얼마나 가야 하는지도 계산을 해야 한다.</p>\n<p>어떤 숫자를 곱하게 되는데 <strong>Learning Rate</strong>라고 하는데 해당 방향으로 얼마나 이동할 것인가를 결정하게 된다.</p>\n<p><strong>랜덤</strong>이라는 위치에서 W가 시작되고 미분을 통해서 Gradient를 구하게 되고 이동을 한다. 그리고 이동한 위치에서 다시 Gradient를 구하게 되고 MInimize를 반복한다.</p>\n<p>Gradient를 어떻게 구할 것인가???</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.41717791411043%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5oTWH/xAAYEAEBAAMAAAAAAAAAAAAAAAABEAARE//aAAgBAQABBQKcxdYT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAEBEgIZH/2gAIAQEABj8CDvYf/8QAHBABAAIBBQAAAAAAAAAAAAAAAQARECExQVGR/9oACAEBAAE/IUb59gsXE2YdzNZfeP/aAAwDAQACAAMAAAAQQM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAQACAwAAAAAAAAAAAAABEQAxUSFBYf/aAAgBAQABPxAilC99BKE7yq6ViDDQ9+eMEHPiYJxv/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/5_4.jpg\"\n        srcset=\"/static/d41d8cd98f00b204e9800998ecf8427e/d2f63/5_4.jpg 163w,\n/static/d41d8cd98f00b204e9800998ecf8427e/c989d/5_4.jpg 325w,\n/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/5_4.jpg 650w,\n/static/d41d8cd98f00b204e9800998ecf8427e/7c09c/5_4.jpg 975w,\n/static/d41d8cd98f00b204e9800998ecf8427e/03213/5_4.jpg 1231w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Chain Rule</h3>\n<p><strong>대부분의 Gradinet를 구하는 식은 Chain Rule에 의해서 계산된다.</strong> 변수가 3개가 있다고 가정하자. y는 x의 functiion z는 y의 function이라고 하자</p>\n<p>여기서 z를 x에 대해서 미분을 하고 싶은 경우...!! 다음과 같은 방법을 진행한다. 각각의 변수를 Layer라고 봐도 무방할 것.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 59.50920245398773%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAIDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdukqCjh/8QAGBABAAMBAAAAAAAAAAAAAAAAAQACECH/2gAIAQEAAQUCcKhF7n//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAZEAADAQEBAAAAAAAAAAAAAAAAAREhMRD/2gAIAQEAAT8hV3rLhYl13ppRKvP/2gAMAwEAAgADAAAAELDP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGRABAQEBAQEAAAAAAAAAAAAAAQARITHR/9oACAEBAAE/EA6EF8F+3BXkMktkqesioEIAbBl//9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/61f62aaad76ea4f5a845abb7ff5c9be1/6aca1/5_5.jpg\"\n        srcset=\"/static/61f62aaad76ea4f5a845abb7ff5c9be1/d2f63/5_5.jpg 163w,\n/static/61f62aaad76ea4f5a845abb7ff5c9be1/c989d/5_5.jpg 325w,\n/static/61f62aaad76ea4f5a845abb7ff5c9be1/6aca1/5_5.jpg 650w,\n/static/61f62aaad76ea4f5a845abb7ff5c9be1/7c09c/5_5.jpg 975w,\n/static/61f62aaad76ea4f5a845abb7ff5c9be1/d14c1/5_5.jpg 1234w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Gradient Descent</h3>\n<p>W에 Gradient를 구하고 learning rate를 곱해서 Gradient minimize를 하게 된다. Error는 output의 function이다. Error를 output에 대해 미분하는 것은 그냥 할 수 있다. 즉, Error을 output에 대해 미분하면 real output - expected output이 된다. = o - d 가 된다라는 것.</p>\n<p>o를 net으로 미분,, net은 activate function에 들어가기 전의 값. 결국 Activate Fucntion의 도함수가 나온다. net을 Weight에 대해 미분을 진행!!</p>\n<p>시그마 wx를 w에 대해 미분을 하게 되면 x가 나온다. 학습식은 다음과 같이 나오게 된다. ==> \"Update rule\"</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.41717791411043%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5oTWH/xAAYEAEBAAMAAAAAAAAAAAAAAAABEAARE//aAAgBAQABBQKcxdYT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAEBEgIZH/2gAIAQEABj8CDvYf/8QAHBABAAIBBQAAAAAAAAAAAAAAAQARECExQVGR/9oACAEBAAE/IUb59gsXE2YdzNZfeP/aAAwDAQACAAMAAAAQQM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAQACAwAAAAAAAAAAAAABEQAxUSFBYf/aAAgBAQABPxAilC99BKE7yq6ViDDQ9+eMEHPiYJxv/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/5_6.jpg\"\n        srcset=\"/static/d41d8cd98f00b204e9800998ecf8427e/d2f63/5_6.jpg 163w,\n/static/d41d8cd98f00b204e9800998ecf8427e/c989d/5_6.jpg 325w,\n/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/5_6.jpg 650w,\n/static/d41d8cd98f00b204e9800998ecf8427e/7c09c/5_6.jpg 975w,\n/static/d41d8cd98f00b204e9800998ecf8427e/cbe08/5_6.jpg 1273w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Learning Rate</h3>\n<p>너무 커도 안돼고 너무 작아도 안된다. 너무 작으면 한 번에 아주 조금씩 가므로 error가 조금씩 줄어든다. 즉 수렴이 느려진다. Local Minimun에 갇힐 수 있는 문제가 발생한다.</p>\n<p>Learning Rate가 많아지면 빠르게 수렴하는 것이 아니라 천천히 수렴한다. 오히려 Error가 커지는 문제가 발생할 수도 있다.</p>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","frontmatter":{"title":"Deep Learning Basic","desc":"Gradient Descent Chain Rule Learning Rate","thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"fixed","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAE/9oADAMBAAIQAxAAAAHm3C2eoP/EABkQAQACAwAAAAAAAAAAAAAAAAEAAhESIP/aAAgBAQABBQIqs0YmOP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAAgITH/2gAIAQEABj8ChhU//8QAGRABAAIDAAAAAAAAAAAAAAAAAQARIDHh/9oACAEBAAE/IdSuC9RFQpw//9oADAMBAAIAAwAAABDED//EABURAQEAAAAAAAAAAAAAAAAAABAh/9oACAEDAQE/EIf/xAAVEQEBAAAAAAAAAAAAAAAAAAAQIf/aAAgBAgEBPxCn/8QAGxABAAEFAQAAAAAAAAAAAAAAATEAEBEhYdH/2gAIAQEAAT8QDYk7CghNHj2k+ISWxy3/2Q=="},"images":{"fallback":{"src":"/static/c131c100e3af8f5788852f15cd55d03c/56315/deeplearning_basic.jpg","srcSet":"/static/c131c100e3af8f5788852f15cd55d03c/56315/deeplearning_basic.jpg 532w","sizes":"532px"},"sources":[{"srcSet":"/static/c131c100e3af8f5788852f15cd55d03c/1239d/deeplearning_basic.webp 532w","type":"image/webp","sizes":"532px"}]},"width":532,"height":363}}},"date":"2022-01-17","category":"Deep Learning"}}},"pageContext":{"slug":"/blog/deeplearning5/"}},
    "staticQueryHashes": ["1840460387"]}