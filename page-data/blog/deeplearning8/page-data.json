{
    "componentChunkName": "component---src-templates-blog-post-tsx",
    "path": "/blog/deeplearning8/",
    "result": {"data":{"markdownRemark":{"html":"<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.87116564417178%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAIEAQX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHt7MpUIH//xAAZEAACAwEAAAAAAAAAAAAAAAAAAQIDESH/2gAIAQEAAQUC7vRQROwVgrNP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFhABAQEAAAAAAAAAAAAAAAAAADEQ/9oACAEBAAY/AtiI/8QAHBAAAwADAAMAAAAAAAAAAAAAAAERIUFRMZGx/9oACAEBAAE/IXoomEa48u+TX10xp17J79H/2gAMAwEAAgADAAAAEGMP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAHhAAAgICAgMAAAAAAAAAAAAAAREAITFhcYGh4fD/2gAIAQEAAT8QUaZMWb8w60+B7mQAxMsygNpWOeoMZkBWvUc0or4p/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/a5937e0bac83bacb068377c6cc9bb9d2/6aca1/8_1.jpg\"\n        srcset=\"/static/a5937e0bac83bacb068377c6cc9bb9d2/d2f63/8_1.jpg 163w,\n/static/a5937e0bac83bacb068377c6cc9bb9d2/c989d/8_1.jpg 325w,\n/static/a5937e0bac83bacb068377c6cc9bb9d2/6aca1/8_1.jpg 650w,\n/static/a5937e0bac83bacb068377c6cc9bb9d2/7c09c/8_1.jpg 975w,\n/static/a5937e0bac83bacb068377c6cc9bb9d2/1a4ae/8_1.jpg 1264w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>MLP Learning</h3>\n<p>Layer 하나를 나타낼 때 x의 super script를 쓰겠다고 가정. X0 는 input vector가 된다. Label에 해당하는 노드는 전부 1</p>\n<p>loss function은 Desired ouput과 real output 사이의 차이에 해당한다. 일단 시작점은 <strong>랜덤</strong>한 node에서 시작해서 조금씩 gradient를 빼줌으로써 학습을 시작한다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 65.03067484662577%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAEDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdxtEywf/8QAGRABAQADAQAAAAAAAAAAAAAAAQACAxAR/9oACAEBAAEFAonVivkc/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFxAAAwEAAAAAAAAAAAAAAAAAARARIP/aAAgBAQAGPwJU4//EABwQAAICAgMAAAAAAAAAAAAAAAABETEQYSGh4f/aAAgBAQABPyF277LejJJL2PZic28f/9oADAMBAAIAAwAAABAAz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABwQAQEAAgIDAAAAAAAAAAAAAAERACEQUTFhgf/aAAgBAQABPxCt1F7GDyHZ7uCg0ruZSvwuNSp0Xj//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/dac89666ca27f174716754955eaefddf/6aca1/8_2.jpg\"\n        srcset=\"/static/dac89666ca27f174716754955eaefddf/d2f63/8_2.jpg 163w,\n/static/dac89666ca27f174716754955eaefddf/c989d/8_2.jpg 325w,\n/static/dac89666ca27f174716754955eaefddf/6aca1/8_2.jpg 650w,\n/static/dac89666ca27f174716754955eaefddf/7c09c/8_2.jpg 975w,\n/static/dac89666ca27f174716754955eaefddf/d14c1/8_2.jpg 1234w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Loss Function(Error Criteria)</h3>\n<p>최종 output Layer에서 desired output 값을 빼고 제곱을 해서 C개에 해당하는 것 만큼 나눠준다. 그러면 Error를 찾을 수 잇고 Mean square를 하게 되는 과정이 된다. <strong>최근에 들어서 Cross entropy function이 많이 사용되고 있다.</strong> Softmax activation과 잘 쓰인다.</p>\n<p>Desired output이 나오고 Real Output에 log가 쓰이고 곱하는 식이 된다. X(n)이 D를 예측하는데 얼마나 도움이 되냐를 표현해주는 식</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 65.6441717791411%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdxJFkB//8QAGBAAAwEBAAAAAAAAAAAAAAAAAAECEhH/2gAIAQEAAQUCaYuingq0aIvS/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGRAAAQUAAAAAAAAAAAAAAAAAAAEQMWFx/9oACAEBAAY/AnWjCD//xAAYEAEBAQEBAAAAAAAAAAAAAAARAQAhMf/aAAgBAQABPyFDLoXukx7XP4EaO3no9z14Ke7/2gAMAwEAAgADAAAAEEPP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAHBABAQACAgMAAAAAAAAAAAAAAREAQSExgcHR/9oACAEBAAE/EHJMdc/cCKo90vvDwkslduSJv1twD2Vvabzb+xy68Z//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/055ca0695949201f6fd5a0f86b177e53/6aca1/8_3.jpg\"\n        srcset=\"/static/055ca0695949201f6fd5a0f86b177e53/d2f63/8_3.jpg 163w,\n/static/055ca0695949201f6fd5a0f86b177e53/c989d/8_3.jpg 325w,\n/static/055ca0695949201f6fd5a0f86b177e53/6aca1/8_3.jpg 650w,\n/static/055ca0695949201f6fd5a0f86b177e53/7c09c/8_3.jpg 975w,\n/static/055ca0695949201f6fd5a0f86b177e53/01ab0/8_3.jpg 1300w,\n/static/055ca0695949201f6fd5a0f86b177e53/a4413/8_3.jpg 1312w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Gradient-based learning</h3>\n<p>Weight에 대해 미분을 하고 현재 weight에서 빼주면서 진행을 하게 된다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.71165644171779%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAIF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB20wWD//EABkQAQEAAwEAAAAAAAAAAAAAAAEAAgMTIv/aAAgBAQABBQJG9XMsswupGwv/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAWEAEBAQAAAAAAAAAAAAAAAAAQITH/2gAIAQEABj8CKYf/xAAbEAEAAgIDAAAAAAAAAAAAAAABABEhMRBBYf/aAAgBAQABPyGxh4GRta3udKz3RBpn/9oADAMBAAIAAwAAABDAz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABwQAQEBAAEFAAAAAAAAAAAAAAERACFBYbHR4f/aAAgBAQABPxC0oi2C+8MBA7p9zUzcmjw7kl+ZhIdeZkhRm//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/2ce1f9b230aa858cf449b614200e31d4/6aca1/8_4.jpg\"\n        srcset=\"/static/2ce1f9b230aa858cf449b614200e31d4/d2f63/8_4.jpg 163w,\n/static/2ce1f9b230aa858cf449b614200e31d4/c989d/8_4.jpg 325w,\n/static/2ce1f9b230aa858cf449b614200e31d4/6aca1/8_4.jpg 650w,\n/static/2ce1f9b230aa858cf449b614200e31d4/7c09c/8_4.jpg 975w,\n/static/2ce1f9b230aa858cf449b614200e31d4/1e760/8_4.jpg 1249w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Back-Progation</h3>\n<p>학습을 할 때 gradient를 구하는데 이는 output에 대해 정의된다라는 것. 그것을 통해 그 이전 것을 계산하게 되는데 이런 알고리즘을 Back Propagation이라고 한다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.48466257668711%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMCBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe3TImsP/8QAGBABAAMBAAAAAAAAAAAAAAAAAQADEEH/2gAIAQEAAQUC7GsXf//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABgQAAIDAAAAAAAAAAAAAAAAABARICGR/9oACAEBAAY/Ag72H//EABkQAQEBAAMAAAAAAAAAAAAAAAEAEVFxof/aAAgBAQABPyHs+2yxOmNjzF//2gAMAwEAAgADAAAAEDMP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGRABAQEBAQEAAAAAAAAAAAAAAREAITGB/9oACAEBAAE/EFjsrfcAnuWFSvRnF5kLI+ap1u//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/b92652808f5b3a421f16525abd71a067/6aca1/8_5.jpg\"\n        srcset=\"/static/b92652808f5b3a421f16525abd71a067/d2f63/8_5.jpg 163w,\n/static/b92652808f5b3a421f16525abd71a067/c989d/8_5.jpg 325w,\n/static/b92652808f5b3a421f16525abd71a067/6aca1/8_5.jpg 650w,\n/static/b92652808f5b3a421f16525abd71a067/7c09c/8_5.jpg 975w,\n/static/b92652808f5b3a421f16525abd71a067/1da99/8_5.jpg 1233w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Matrix Notation</h3>\n<p>y 가 weighted node의 Summation에 대한 값으로 표현하게 된다. 여기선 a를 net value라고 하고 해당 입력을 벡터에 대해서 표현하고자 한다. 복잡했던 notaion이 간단하게 표현이 가능하게 된다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 65.03067484662577%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAABAAMBAAAAAAAAAAAAAAAAAAECAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHuyFGg/8QAFxAAAwEAAAAAAAAAAAAAAAAAARASIP/aAAgBAQABBQJQMf/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABYQAAMAAAAAAAAAAAAAAAAAABAgQf/aAAgBAQAGPwIVP//EABkQAAMBAQEAAAAAAAAAAAAAAAABESExEP/aAAgBAQABPyGO2iZs3dO9Gbe+f//aAAwDAQACAAMAAAAQIM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAAIDAQAAAAAAAAAAAAABEQAhEDFRYf/aAAgBAQABPxBBh+xsxFdjHeFE7ZvlyjMDQtnzvj//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/3765084d2ab0aab4a542b9006699439f/6aca1/8_6.jpg\"\n        srcset=\"/static/3765084d2ab0aab4a542b9006699439f/d2f63/8_6.jpg 163w,\n/static/3765084d2ab0aab4a542b9006699439f/c989d/8_6.jpg 325w,\n/static/3765084d2ab0aab4a542b9006699439f/6aca1/8_6.jpg 650w,\n/static/3765084d2ab0aab4a542b9006699439f/7c09c/8_6.jpg 975w,\n/static/3765084d2ab0aab4a542b9006699439f/e6205/8_6.jpg 1255w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Chain Rule</h3>\n<p>x에서 y, z로 갈때 먼저 z를 y로 미분하고 y를 x 에 대해 미분한 것을 곱해주면 된다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 63.80368098159509%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAgADBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe4ggWsf/8QAGBAAAgMAAAAAAAAAAAAAAAAAAAEQEkH/2gAIAQEAAQUC0oOf/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFxAAAwEAAAAAAAAAAAAAAAAAARAgMf/aAAgBAQAGPwJaY//EABkQAAMBAQEAAAAAAAAAAAAAAAABETFBUf/aAAgBAQABPyF3opFb1fpoamCP/9oADAMBAAIAAwAAABDjD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABwQAQEBAAIDAQAAAAAAAAAAAAERACExQWGBcf/aAAgBAQABPxCCT4vGF5j+YVTNO/GkZO3MAU+XAL7bv//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/86a5a5af05a1234b29feb718a93b9538/6aca1/8_7.jpg\"\n        srcset=\"/static/86a5a5af05a1234b29feb718a93b9538/d2f63/8_7.jpg 163w,\n/static/86a5a5af05a1234b29feb718a93b9538/c989d/8_7.jpg 325w,\n/static/86a5a5af05a1234b29feb718a93b9538/6aca1/8_7.jpg 650w,\n/static/86a5a5af05a1234b29feb718a93b9538/7c09c/8_7.jpg 975w,\n/static/86a5a5af05a1234b29feb718a93b9538/efc8b/8_7.jpg 1240w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Chain Rule</h3>\n<p>Loss를 Output z에 대해서 미분을 해버린 것으로부터 시작한다. z를 x에 대해 미분한 것과 z를 y에 대해 미분한 것만 있으면 할 수 있다는 것.</p>\n<p><strong>gradient를 받게 되고 자신에게 필요한 gradient를 가질 수 있다는 것이 key point</strong></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 67.48466257668711%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe3QS0H/xAAZEAACAwEAAAAAAAAAAAAAAAAQEQACEkH/2gAIAQEAAQUC65ioQ//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABYQAAMAAAAAAAAAAAAAAAAAABAgMf/aAAgBAQAGPwIRP//EABkQAQEAAwEAAAAAAAAAAAAAAAEhABExEP/aAAgBAQABPyFieYRXCrrW4m3s9f/aAAwDAQACAAMAAAAQ88//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEAAwEAAwAAAAAAAAAAAAABABExIVFxkf/aAAgBAQABPxAmw0a3zCOj2y0Cti22K4rHCJSufIAHAn//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/5afdc44421b82cda674488c852f6c4b2/6aca1/8_8.jpg\"\n        srcset=\"/static/5afdc44421b82cda674488c852f6c4b2/d2f63/8_8.jpg 163w,\n/static/5afdc44421b82cda674488c852f6c4b2/c989d/8_8.jpg 325w,\n/static/5afdc44421b82cda674488c852f6c4b2/6aca1/8_8.jpg 650w,\n/static/5afdc44421b82cda674488c852f6c4b2/7c09c/8_8.jpg 975w,\n/static/5afdc44421b82cda674488c852f6c4b2/63bea/8_8.jpg 1251w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Chain Rule</h3>\n<p>두 개의 vector의 편미분의 합을 구하면 <strong>각각의 경우</strong>를 구할 수 있다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 63.190184049079754%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAIDAQX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHuMmky4f/EABsQAAICAwEAAAAAAAAAAAAAAAECADEDEBEh/9oACAEBAAEFAqnsOFCWXprX/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAwEBAAAAAAAAAAAAAAAAAAEQETH/2gAIAQEABj8C7NaEZP/EAB0QAQABAwUAAAAAAAAAAAAAAAEAEBExIVFhcaH/2gAIAQEAAT8hdZubE7eRlcLzCdMAYp//2gAMAwEAAgADAAAAEBPP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGxABAQADAQEBAAAAAAAAAAAAAREAITFREGH/2gAIAQEAAT8QiqHkLvvv5k8cI2VVrFn0Y92R13AUMBDVh8//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/f572ee837e551bf2645b58a5ef3669ad/6aca1/8_9.jpg\"\n        srcset=\"/static/f572ee837e551bf2645b58a5ef3669ad/d2f63/8_9.jpg 163w,\n/static/f572ee837e551bf2645b58a5ef3669ad/c989d/8_9.jpg 325w,\n/static/f572ee837e551bf2645b58a5ef3669ad/6aca1/8_9.jpg 650w,\n/static/f572ee837e551bf2645b58a5ef3669ad/7c09c/8_9.jpg 975w,\n/static/f572ee837e551bf2645b58a5ef3669ad/e4b8a/8_9.jpg 1207w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span>\r\n일반화시키면 2개 이상인 경우에도 구할 수 있다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 63.80368098159509%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe6mjFj/xAAYEAEBAAMAAAAAAAAAAAAAAAABAAIhQf/aAAgBAQABBQLsYksbL//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABYQAAMAAAAAAAAAAAAAAAAAABAgQf/aAAgBAQAGPwIVP//EABwQAAMAAgMBAAAAAAAAAAAAAAABESExQVFh4f/aAAgBAQABPyHKornwWvZ3LPHRykaP/9oADAMBAAIAAwAAABBwz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABsQAQEBAAIDAAAAAAAAAAAAAAERADFRIYGh/9oACAEBAAE/EERTPbmBPuAoUsqc4qj2c5FPB1gBAhv/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/d6382e2c58229da8143ba5a87811d6ec/6aca1/8_10.jpg\"\n        srcset=\"/static/d6382e2c58229da8143ba5a87811d6ec/d2f63/8_10.jpg 163w,\n/static/d6382e2c58229da8143ba5a87811d6ec/c989d/8_10.jpg 325w,\n/static/d6382e2c58229da8143ba5a87811d6ec/6aca1/8_10.jpg 650w,\n/static/d6382e2c58229da8143ba5a87811d6ec/7c09c/8_10.jpg 975w,\n/static/d6382e2c58229da8143ba5a87811d6ec/4ac4c/8_10.jpg 1243w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<p>델타에 x를 붙이면 어떤 식을 x에 대해 미분한 값이다라는 뜻.Z를 y에 대해 미분하고 Z를 x에 대해 미분한 것</p>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","frontmatter":{"title":"Deep Learning Basic","desc":"MLP Learning Loss Function Back Propagation Chain Rule","thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"fixed","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAE/9oADAMBAAIQAxAAAAHm3C2eoP/EABkQAQACAwAAAAAAAAAAAAAAAAEAAhESIP/aAAgBAQABBQIqs0YmOP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAAgITH/2gAIAQEABj8ChhU//8QAGRABAAIDAAAAAAAAAAAAAAAAAQARIDHh/9oACAEBAAE/IdSuC9RFQpw//9oADAMBAAIAAwAAABDED//EABURAQEAAAAAAAAAAAAAAAAAABAh/9oACAEDAQE/EIf/xAAVEQEBAAAAAAAAAAAAAAAAAAAQIf/aAAgBAgEBPxCn/8QAGxABAAEFAQAAAAAAAAAAAAAAATEAEBEhYdH/2gAIAQEAAT8QDYk7CghNHj2k+ISWxy3/2Q=="},"images":{"fallback":{"src":"/static/c131c100e3af8f5788852f15cd55d03c/56315/deeplearning_basic.jpg","srcSet":"/static/c131c100e3af8f5788852f15cd55d03c/56315/deeplearning_basic.jpg 532w","sizes":"532px"},"sources":[{"srcSet":"/static/c131c100e3af8f5788852f15cd55d03c/1239d/deeplearning_basic.webp 532w","type":"image/webp","sizes":"532px"}]},"width":532,"height":363}}},"date":"2022-01-17","category":"Deep Learning"}}},"pageContext":{"slug":"/blog/deeplearning8/"}},
    "staticQueryHashes": ["1840460387"]}