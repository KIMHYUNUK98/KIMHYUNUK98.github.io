{
    "componentChunkName": "component---src-templates-blog-post-tsx",
    "path": "/blog/deeplearning9/",
    "result": {"data":{"markdownRemark":{"html":"<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.41717791411043%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5oTWH/xAAYEAEBAAMAAAAAAAAAAAAAAAABEAARE//aAAgBAQABBQKcxdYT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAEBEgIZH/2gAIAQEABj8CDvYf/8QAHBABAAIBBQAAAAAAAAAAAAAAAQARECExQVGR/9oACAEBAAE/IUb59gsXE2YdzNZfeP/aAAwDAQACAAMAAAAQQM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAQACAwAAAAAAAAAAAAABEQAxUSFBYf/aAAgBAQABPxAilC99BKE7yq6ViDDQ9+eMEHPiYJxv/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/9_1.jpg\"\n        srcset=\"/static/d41d8cd98f00b204e9800998ecf8427e/d2f63/9_1.jpg 163w,\n/static/d41d8cd98f00b204e9800998ecf8427e/c989d/9_1.jpg 325w,\n/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/9_1.jpg 650w,\n/static/d41d8cd98f00b204e9800998ecf8427e/7c09c/9_1.jpg 975w,\n/static/d41d8cd98f00b204e9800998ecf8427e/9465e/9_1.jpg 1257w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Gradient and Jacobian</h3>\n<p>Gradient Vector 스칼라 값을 벡터로 미분하면 얻어지는 것. Error function을 벡터로 미분하면 Gradient를 얻는다.</p>\n<p><strong>벡터를 벡터로 미분하는 경우</strong> Matrix 형태로 나타나게 되는데 그것을 Jacobian Matrix 라고 한다.</p>\n<p><strong>output의 개수와 Input의 개수로의 행열로 이루어진 Matrix</strong>가 된다. 벡터끼리의 Chain rule은 Matrix의 형태로 나타난다고 볼 수 있다. <strong>출력값에 대한 Gradient에 Jacobian 행렬을 곱하면 Input값에 대한 Gradient가 나온다.</strong></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.09815950920245%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdpjJLD/xAAaEAEAAQUAAAAAAAAAAAAAAAABAAIQESEx/9oACAEBAAEFAnM3CgjY5//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAABARIFH/2gAIAQEABj8CD2P/xAAaEAADAAMBAAAAAAAAAAAAAAAAAREhMUFh/9oACAEBAAE/IemlfRoMVrctkcuxz0xH/9oADAMBAAIAAwAAABBgD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EAB0QAQEBAAICAwAAAAAAAAAAAAERACExQWFxobH/2gAIAQEAAT8QJWugVr6cNI9FVWfuOAkrn5whELTy5Kxv5ygNZ7u//9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/95d6a78372badb503c1ef871581daac5/6aca1/9_2.jpg\"\n        srcset=\"/static/95d6a78372badb503c1ef871581daac5/d2f63/9_2.jpg 163w,\n/static/95d6a78372badb503c1ef871581daac5/c989d/9_2.jpg 325w,\n/static/95d6a78372badb503c1ef871581daac5/6aca1/9_2.jpg 650w,\n/static/95d6a78372badb503c1ef871581daac5/7c09c/9_2.jpg 975w,\n/static/95d6a78372badb503c1ef871581daac5/1a4ae/9_2.jpg 1264w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Back-Propagation on NEaral Nets</h3>\n<p>밑의 layer에서 벡터가 올라오면 Weighted Sum을 해서 Output을 내주게 되는데 Desired Ouput과의 Loss 를 구하게 된다. <strong>학습의 경우 Loss Function을 Output Function에 대해서 미분을</strong> 하게 된다.  두 가지의 경우를 계산하게 되는 데 Loss 를 Weight로 미분한 Gradient가 첫 번째인데, 이것이 필요한 이유는 <strong>Weight을 업데이트</strong>할 떄 직접적으로 사용되는 값이다. 다른 하나는 Loss를 X의 N-1 에 대한 Gradient를 계산해야 한다. 이것은 <strong>밑의 Layer가 학습하기 위해 계산을 해줘야 하는 것</strong>이다.</p>\n<p>Output Layer에 대한 Loss 가 직접적으로 계산이 안되기 때문에 입력에 대한 Gradient를 만들어주고 마치 밑의 Layer의 출력 Gradient를 구하는 결과와 비슷하게 보인다.</p>\n<p>출력 Gradient를 받아서 Weight에 대한 Gradient를 계산하고 자기 Weight에 대한 것을 확인한다. 출력 Gradient로 부터 입력 Gradient를 계산해서 밑의 Layer에 넘겨주게 되고 동일한 절차로 자신의 Node의 Weight의 Gardient를 계산하는 절차가 진행된다. 이렇게 학습이 반복되며 이 과정을 <strong>Back Propagation</strong>이라고 한다.</p>\n<p>Loss 를 Weight에 대해 미분할 때 먼저 Loss를 X에 대해 미분하고 X를 Weight에 대해 미분해서 곱하게 된다. 내려올 경우 Loss를 입력으로 미분할 경우 Loss를 입력으로 미분하고 Output을 X에 대해 미분하게 된다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.71165644171779%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAMBAgX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHblax5QP/EABkQAQEBAAMAAAAAAAAAAAAAAAECAAMRMf/aAAgBAQABBQLeYkzyGblZs6//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAXEAEBAQEAAAAAAAAAAAAAAAAAQRAh/9oACAEBAAY/Aq7tV//EABoQAQACAwEAAAAAAAAAAAAAAAEAESExQbH/2gAIAQEAAT8hRuB6Zgrytt7geMvHiaYj/9oADAMBAAIAAwAAABDjz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABwQAQEAAgIDAAAAAAAAAAAAAAERACExUUFxsf/aAAgBAQABPxBjKN9w+4CThHly9FMq+cZa8Y6MuoYE07941EgvIZ//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/256cdc46c48fe866252c14ff41440660/6aca1/9_3.jpg\"\n        srcset=\"/static/256cdc46c48fe866252c14ff41440660/d2f63/9_3.jpg 163w,\n/static/256cdc46c48fe866252c14ff41440660/c989d/9_3.jpg 325w,\n/static/256cdc46c48fe866252c14ff41440660/6aca1/9_3.jpg 650w,\n/static/256cdc46c48fe866252c14ff41440660/7c09c/9_3.jpg 975w,\n/static/256cdc46c48fe866252c14ff41440660/34898/9_3.jpg 1270w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Back-Propagation on MLP</h3>\n<p>Loss function은 MSE Function을 이용한다고 가정(미분이 쉬움) Training Algorithm은 먼저 Gradient를 계산을 한다. Gradient Descent에 대한 일반 식.\r\nW: 두 번째 Layer의 Weight V: 첫 번째 Layer의 Weight</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 68.71165644171779%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdwnWViQ/8QAGBAAAwEBAAAAAAAAAAAAAAAAAAECAxL/2gAIAQEAAQUCYmcIveRXMi3ln//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABoQAAICAwAAAAAAAAAAAAAAAAABESESMUH/2gAIAQEABj8CIOjVmdwaZ//EABkQAAMBAQEAAAAAAAAAAAAAAAABESExkf/aAAgBAQABPyFHbcE3VNm7p3oioZzggQE9Xif/2gAMAwEAAgADAAAAEJDP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAHBABAQEAAQUAAAAAAAAAAAAAAREAIVFhcYHR/9oACAEBAAE/EALj7lz52qYFSNnPIuypB185ijSoQsfecAAzkfd//9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/77768f51dcb9cc8a84f45b3cbaa4f194/6aca1/9_4.jpg\"\n        srcset=\"/static/77768f51dcb9cc8a84f45b3cbaa4f194/d2f63/9_4.jpg 163w,\n/static/77768f51dcb9cc8a84f45b3cbaa4f194/c989d/9_4.jpg 325w,\n/static/77768f51dcb9cc8a84f45b3cbaa4f194/6aca1/9_4.jpg 650w,\n/static/77768f51dcb9cc8a84f45b3cbaa4f194/7c09c/9_4.jpg 975w,\n/static/77768f51dcb9cc8a84f45b3cbaa4f194/cbe08/9_4.jpg 1273w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Training of 2nd Layer</h3>\n<p>먼저 뒤쪽 Gardient 계산이 필요하다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.25766871165644%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAIDAQX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHubBS4gf/EABsQAAICAwEAAAAAAAAAAAAAAAIRAAEDEyEx/9oACAEBAAEFAr96hFQsivbAN1//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAXEAEAAwAAAAAAAAAAAAAAAAAQADEy/9oACAEBAAY/Ampk/8QAGxAAAwACAwAAAAAAAAAAAAAAAAEREDFBUfD/2gAIAQEAAT8hrkdBA63W7vAl6YyrR//aAAwDAQACAAMAAAAQgw//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAeEAEAAgIBBQAAAAAAAAAAAAABABEhQTFRYZHh8f/aAAgBAQABPxAcC8wojJ03LQlSuLwC0pd+oWVC9fiMkLe3tP/Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/069662185105c4d6ce19e7c9b0ee36c3/6aca1/9_5.jpg\"\n        srcset=\"/static/069662185105c4d6ce19e7c9b0ee36c3/d2f63/9_5.jpg 163w,\n/static/069662185105c4d6ce19e7c9b0ee36c3/c989d/9_5.jpg 325w,\n/static/069662185105c4d6ce19e7c9b0ee36c3/6aca1/9_5.jpg 650w,\n/static/069662185105c4d6ce19e7c9b0ee36c3/7c09c/9_5.jpg 975w,\n/static/069662185105c4d6ce19e7c9b0ee36c3/01ab0/9_5.jpg 1300w,\n/static/069662185105c4d6ce19e7c9b0ee36c3/72a7d/9_5.jpg 1306w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Training of 2nd Layer</h3>\n<p><strong>먼저 Error를 Output에 대해 미분하고  Output을 Net Value에 대해 미분하고 Net Value를 Weight에 대해 미분한다.</strong></p>\n<p>W가 Matrix인 경우 미분이 잘 안돼기 때문에 <strong>W를 일렬로 나열해서 미분을 하겠다</strong>라는 것이 된다. Output을 Net 에 대해 미분한 Jacobin Matrix를 생각해보자. 이는 <strong>Diagonal Matrix</strong>밖에 남지 않는다. Diagonal이 아닌 Matirx에 대한 미분값은 다 0이 나오게 된다. 도함수는 Diagonal matrix가 된다는 것이다.</p>\n<p>마지막으로 Net 을 Weight에 대해 미분한 것 -> Row의 계수는 output 개수 column의 개수는 input의 개수가 된다. 이를 수식으로 나타내면 아래와 같은 수식을 얻을 수 있다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 66.87116564417178%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAQBAgX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHblSg8LB//xAAbEAABBAMAAAAAAAAAAAAAAAABAAIDERQhQf/aAAgBAQABBQLqDQE6ejkaZLY//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFxABAQEBAAAAAAAAAAAAAAAAADIQMf/aAAgBAQAGPwLZS4//xAAbEAEAAgMBAQAAAAAAAAAAAAABABEhMVFhsf/aAAgBAQABPyFMoWGyGONtssePY6PuXD32f//aAAwDAQACAAMAAAAQwM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAdEAEBAAEEAwAAAAAAAAAAAAABEQAhMUFhofDx/9oACAEBAAE/ENgfLlcEOK7ZJCKalriBBqT1Muo15+cm0974Os//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/37256ad10007fc08444e772eb350ea56/6aca1/9_6.jpg\"\n        srcset=\"/static/37256ad10007fc08444e772eb350ea56/d2f63/9_6.jpg 163w,\n/static/37256ad10007fc08444e772eb350ea56/c989d/9_6.jpg 325w,\n/static/37256ad10007fc08444e772eb350ea56/6aca1/9_6.jpg 650w,\n/static/37256ad10007fc08444e772eb350ea56/7c09c/9_6.jpg 975w,\n/static/37256ad10007fc08444e772eb350ea56/cbe08/9_6.jpg 1273w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Gradient for Back-propagation</h3>\n<p>Error를 Y 로 미분한 것이 필요하다. Net Value를 Y에 대해 미분한다는 것이 이전과 다른 점이 된다. Net을 Y로 미분하면 W가 나온다는 것.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.41717791411043%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5oTWH/xAAYEAEBAAMAAAAAAAAAAAAAAAABEAARE//aAAgBAQABBQKcxdYT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAEBEgIZH/2gAIAQEABj8CDvYf/8QAHBABAAIBBQAAAAAAAAAAAAAAAQARECExQVGR/9oACAEBAAE/IUb59gsXE2YdzNZfeP/aAAwDAQACAAMAAAAQQM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAQACAwAAAAAAAAAAAAABEQAxUSFBYf/aAAgBAQABPxAilC99BKE7yq6ViDDQ9+eMEHPiYJxv/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/9_7.jpg\"\n        srcset=\"/static/d41d8cd98f00b204e9800998ecf8427e/d2f63/9_7.jpg 163w,\n/static/d41d8cd98f00b204e9800998ecf8427e/c989d/9_7.jpg 325w,\n/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/9_7.jpg 650w,\n/static/d41d8cd98f00b204e9800998ecf8427e/7c09c/9_7.jpg 975w,\n/static/d41d8cd98f00b204e9800998ecf8427e/9a8cd/9_7.jpg 1248w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Training of 2nd-Layor</h3>\n<p>Error를 W로 미분하는 것은 자신의 Node를 학습할 때 사용. Error를 Y로 미분하는 것은 다음 Layer로 전달할 때 필요. 이렇게 하면 두 번째 Layer에 대한 학습이 완료되는 것이다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 65.6441717791411%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMEBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe4lM1Jj/8QAGhAAAgMBAQAAAAAAAAAAAAAAAAECERIDIf/aAAgBAQABBQJ+EXYo0SlZrDXSz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABgQAQEAAwAAAAAAAAAAAAAAABEAEBIx/9oACAEBAAY/AlzrBcv/xAAbEAADAQADAQAAAAAAAAAAAAAAAREhMUGBsf/aAAgBAQABPyHmZkhKY6IqXXdZrjuUmSvSW/R//9oADAMBAAIAAwAAABDAD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABsQAAIDAAMAAAAAAAAAAAAAAAERACExYYHw/9oACAEBAAE/EKgg0CIlZi9gthsJhuNV3o5gLknrpvUe0orxT//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/dc6eed69c6256bbaabb8bfd0abb3f1de/6aca1/9_8.jpg\"\n        srcset=\"/static/dc6eed69c6256bbaabb8bfd0abb3f1de/d2f63/9_8.jpg 163w,\n/static/dc6eed69c6256bbaabb8bfd0abb3f1de/c989d/9_8.jpg 325w,\n/static/dc6eed69c6256bbaabb8bfd0abb3f1de/6aca1/9_8.jpg 650w,\n/static/dc6eed69c6256bbaabb8bfd0abb3f1de/7c09c/9_8.jpg 975w,\n/static/dc6eed69c6256bbaabb8bfd0abb3f1de/ac614/9_8.jpg 1272w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Training of 1st Layer</h3>\n<p>첫 번째 Layer의 학습을 어떻게 하는가? Error를 V로 미분해야 한다.</p>\n<p>Error를 y로 미분한 벡터는 아까 계산을 했었다. Error을 output으로 미분한 벡터를 Back Propagtion을 하므로써 Y를 계산했다. 우선 Y로 계산한 것들은 가지고 있다고 가정을 할 수 있게 된다. <strong>Y에 대한 Gradient는 Back Propagtion을 통해서 건너 받은 벡터</strong>가 된다.</p>\n<p>Diagonal Matrix 부분에는 <strong>Net Value에 대한 Y의 도함수</strong>가 된다. Net의 V에 대한 미분 행렬의 경우 아까와 비슷하다. <strong>Error를 Y에 대해 미분하면 앞의 경우에서 구한 수식을 가져다가 사용하면 된다.</strong></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 63.80368098159509%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAQAF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB3IAmP//EABoQAAICAwAAAAAAAAAAAAAAAAERAEESITH/2gAIAQEAAQUCvawlnga//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFxABAAMAAAAAAAAAAAAAAAAAEAAxQf/aAAgBAQAGPwI1uf/EABoQAQADAAMAAAAAAAAAAAAAAAEAESEQMaH/2gAIAQEAAT8hbtvvAHV0t9xlarGCH//aAAwDAQACAAMAAAAQYA//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEAAgIDAAAAAAAAAAAAAAABABEhYTFBgf/aAAgBAQABPxA4CjFAK9Ny+1OUsGV7eoxyis3uUVtvU//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/b08a9867d423b836044c7fd293940cec/6aca1/9_9.jpg\"\n        srcset=\"/static/b08a9867d423b836044c7fd293940cec/d2f63/9_9.jpg 163w,\n/static/b08a9867d423b836044c7fd293940cec/c989d/9_9.jpg 325w,\n/static/b08a9867d423b836044c7fd293940cec/6aca1/9_9.jpg 650w,\n/static/b08a9867d423b836044c7fd293940cec/7c09c/9_9.jpg 975w,\n/static/b08a9867d423b836044c7fd293940cec/a42c7/9_9.jpg 1263w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","frontmatter":{"title":"Deep Learning Basic","desc":"Jacobian Back-Propagation Training of Layor","thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"fixed","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAE/9oADAMBAAIQAxAAAAHm3C2eoP/EABkQAQACAwAAAAAAAAAAAAAAAAEAAhESIP/aAAgBAQABBQIqs0YmOP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAAgITH/2gAIAQEABj8ChhU//8QAGRABAAIDAAAAAAAAAAAAAAAAAQARIDHh/9oACAEBAAE/IdSuC9RFQpw//9oADAMBAAIAAwAAABDED//EABURAQEAAAAAAAAAAAAAAAAAABAh/9oACAEDAQE/EIf/xAAVEQEBAAAAAAAAAAAAAAAAAAAQIf/aAAgBAgEBPxCn/8QAGxABAAEFAQAAAAAAAAAAAAAAATEAEBEhYdH/2gAIAQEAAT8QDYk7CghNHj2k+ISWxy3/2Q=="},"images":{"fallback":{"src":"/static/c131c100e3af8f5788852f15cd55d03c/56315/deeplearning_basic.jpg","srcSet":"/static/c131c100e3af8f5788852f15cd55d03c/56315/deeplearning_basic.jpg 532w","sizes":"532px"},"sources":[{"srcSet":"/static/c131c100e3af8f5788852f15cd55d03c/1239d/deeplearning_basic.webp 532w","type":"image/webp","sizes":"532px"}]},"width":532,"height":363}}},"date":"2022-01-17","category":"Deep Learning"}}},"pageContext":{"slug":"/blog/deeplearning9/"}},
    "staticQueryHashes": ["1840460387"]}