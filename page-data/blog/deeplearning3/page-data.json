{
    "componentChunkName": "component---src-templates-blog-post-tsx",
    "path": "/blog/deeplearning3/",
    "result": {"data":{"markdownRemark":{"html":"<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.41717791411043%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5oTWH/xAAYEAEBAAMAAAAAAAAAAAAAAAABEAARE//aAAgBAQABBQKcxdYT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAEBEgIZH/2gAIAQEABj8CDvYf/8QAHBABAAIBBQAAAAAAAAAAAAAAAQARECExQVGR/9oACAEBAAE/IUb59gsXE2YdzNZfeP/aAAwDAQACAAMAAAAQQM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAQACAwAAAAAAAAAAAAABEQAxUSFBYf/aAAgBAQABPxAilC99BKE7yq6ViDDQ9+eMEHPiYJxv/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/3_1.jpg\"\n        srcset=\"/static/d41d8cd98f00b204e9800998ecf8427e/d2f63/3_1.jpg 163w,\n/static/d41d8cd98f00b204e9800998ecf8427e/c989d/3_1.jpg 325w,\n/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/3_1.jpg 650w,\n/static/d41d8cd98f00b204e9800998ecf8427e/7c09c/3_1.jpg 975w,\n/static/d41d8cd98f00b204e9800998ecf8427e/ae305/3_1.jpg 1164w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Neural Network</h3>\n<p>생물학적인 뇌세포를 모방해서 만든 Computation Model.  앞 쪽에서 신호를 받아서 Merge하고 일정 수치 이상이면 다음으로 넘기는 역할로 알려져 있다.</p>\n<p>사람의 뇌는 복잡한 일을 할 수 있지만 뉴런 하나의 단위는 굉장히 단순하다는 것. 이런 개념으로 컴퓨팅을 하기 위한 모델링을 시작했다.</p>\n<p>Connection Weight을 곱해서 각 데이터를 더해서 Merge해주고 Activation Function 을 통해서 함수가 만들어진다. 모든 값이 시그마로 인해 더해지고 (세타)라는 값을 더하게 된다. 이 (세타)의 경우 직선의 방정식 <strong>y = ax + b에서 b가 있어야 모든 직선이 표현되듯</strong> Bias를 (세타)로 더하게 되는 것이다.</p>\n<p>뉴런 하나가 하는 것을 weighted sum의 형태로 Merge하는 것, 입력 정보를 Merge해서 좀 더 High Level 로 바꾸는 역할을 한다.</p>\n<p>Nearal Network에서 가장 중요한 것은 weight를 얼마로 해야하는 지가 중요하다. 이렇게 생긴 뉴런이 여러개를 구성하면서 여러개의 Layer가 구성되고 이렇게 하나의 Network가 구성된다.</p>\n<p>Weight라는 것은 노드와 노드를 연결하는 집합체가 된다. Neural Network가 어떤 일을 배웠다라는 것은 그 Task를 수행하면서 가장 최적의 weight를 찾았다라는 것이다. 무엇인가를 배웠다라고 할 때 Knowledge가 weight에 저장된다고 할 수 있다. Connection Weight은 Training data에 의해서 자동으로 최적화 할 수 있다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.41717791411043%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5oTWH/xAAYEAEBAAMAAAAAAAAAAAAAAAABEAARE//aAAgBAQABBQKcxdYT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAEBEgIZH/2gAIAQEABj8CDvYf/8QAHBABAAIBBQAAAAAAAAAAAAAAAQARECExQVGR/9oACAEBAAE/IUb59gsXE2YdzNZfeP/aAAwDAQACAAMAAAAQQM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAQACAwAAAAAAAAAAAAABEQAxUSFBYf/aAAgBAQABPxAilC99BKE7yq6ViDDQ9+eMEHPiYJxv/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/3_2.jpg\"\n        srcset=\"/static/d41d8cd98f00b204e9800998ecf8427e/d2f63/3_2.jpg 163w,\n/static/d41d8cd98f00b204e9800998ecf8427e/c989d/3_2.jpg 325w,\n/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/3_2.jpg 650w,\n/static/d41d8cd98f00b204e9800998ecf8427e/7c09c/3_2.jpg 975w,\n/static/d41d8cd98f00b204e9800998ecf8427e/f7fe6/3_2.jpg 1258w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Neural Network</h3>\n<p>Parameter에 해당하는 것이 Weight와 Bias 두 가지 종류의 숫자가 Behavior를 결정할 수 있는 파라미터가 된다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.41717791411043%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5oTWH/xAAYEAEBAAMAAAAAAAAAAAAAAAABEAARE//aAAgBAQABBQKcxdYT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAEBEgIZH/2gAIAQEABj8CDvYf/8QAHBABAAIBBQAAAAAAAAAAAAAAAQARECExQVGR/9oACAEBAAE/IUb59gsXE2YdzNZfeP/aAAwDAQACAAMAAAAQQM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAQACAwAAAAAAAAAAAAABEQAxUSFBYf/aAAgBAQABPxAilC99BKE7yq6ViDDQ9+eMEHPiYJxv/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/3_3.jpg\"\n        srcset=\"/static/d41d8cd98f00b204e9800998ecf8427e/d2f63/3_3.jpg 163w,\n/static/d41d8cd98f00b204e9800998ecf8427e/c989d/3_3.jpg 325w,\n/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/3_3.jpg 650w,\n/static/d41d8cd98f00b204e9800998ecf8427e/7c09c/3_3.jpg 975w,\n/static/d41d8cd98f00b204e9800998ecf8427e/deb44/3_3.jpg 1212w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Learning Mapping</h3>\n<p>input vector가 들어왔을 때 그에 대응하는 output vector를 낼 수 있도록 학습을 한다. 세상에 존재하는 많은 문제가 벡터에서 벡터로 Mapping하는 것으로 표현할 수 있다. <strong>어떤 물체를 식별했을 때 맞추는 문제도 벡터로 표현 할 수 있다.</strong> 입력 변수가 들어왔을 때 출력 벡터로 얻게 되는 것도 다 가능하다.</p>\n<p>최근에는 Sequence에서 Sequence로 이동하는 것까지 구현이 된다. 하지만 Sequence는 길이가 고정되어 있지 않다.</p>\n<p>전통적인 방법은 Vector에서 Vector로 Mapping을 했지만 <strong>요즘은 Seq에서 Seq로 연결</strong>까지 진행하고 있다. 어떤 Text가 들어왔을 때 음성신호로 바꾸면 TTS가 되고 반대의 경우 ASR이 된다. 이도 마찬가지로 Mapping의 문제이다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.41717791411043%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5oTWH/xAAYEAEBAAMAAAAAAAAAAAAAAAABEAARE//aAAgBAQABBQKcxdYT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAEBEgIZH/2gAIAQEABj8CDvYf/8QAHBABAAIBBQAAAAAAAAAAAAAAAQARECExQVGR/9oACAEBAAE/IUb59gsXE2YdzNZfeP/aAAwDAQACAAMAAAAQQM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAQACAwAAAAAAAAAAAAABEQAxUSFBYf/aAAgBAQABPxAilC99BKE7yq6ViDDQ9+eMEHPiYJxv/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/3_4.jpg\"\n        srcset=\"/static/d41d8cd98f00b204e9800998ecf8427e/d2f63/3_4.jpg 163w,\n/static/d41d8cd98f00b204e9800998ecf8427e/c989d/3_4.jpg 325w,\n/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/3_4.jpg 650w,\n/static/d41d8cd98f00b204e9800998ecf8427e/7c09c/3_4.jpg 975w,\n/static/d41d8cd98f00b204e9800998ecf8427e/efc8b/3_4.jpg 1240w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Learn Probability distribution</h3>\n<p>Neural Network한테 사람의 얼굴을 보여주고 사람을 만들어라 하며 비슷한 패턴의 사람을 만들어낼 수 있다는 것이다. 사람 얼굴이 가지고 있는 확률 분포를 자신이 학습한다는 것이다. 그러한 <strong>확률 분포에 입각해서 새로운 output을 만들어내는 것이 Neural Network의 역할</strong>이라고 할 수 있다. 확률분포를 학습했을 떄 할 수 있는 것들이 있다.</p>\n<h3>Sample generation과 Restoration</h3>\n<p>왼쪽 눈을 보여주고 나머지를 그리라고 했을 때 복구할 수 있다는 것이다. Transformation, 흑백 영상을 주고 칼라 영상을 만들어라라고 하는 것.\r\n학습 없이 하라는 것은 불가능 하지만 많은 영상을 보여주고 나서 많은 확률 분포에 입각하여 Output을 내게 되는 것이다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.41717791411043%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe5oTWH/xAAYEAEBAAMAAAAAAAAAAAAAAAABEAARE//aAAgBAQABBQKcxdYT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGBAAAgMAAAAAAAAAAAAAAAAAEBEgIZH/2gAIAQEABj8CDvYf/8QAHBABAAIBBQAAAAAAAAAAAAAAAQARECExQVGR/9oACAEBAAE/IUb59gsXE2YdzNZfeP/aAAwDAQACAAMAAAAQQM//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEBAQACAwAAAAAAAAAAAAABEQAxUSFBYf/aAAgBAQABPxAilC99BKE7yq6ViDDQ9+eMEHPiYJxv/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"img\"\n        title=\"img\"\n        src=\"/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/3_5.jpg\"\n        srcset=\"/static/d41d8cd98f00b204e9800998ecf8427e/d2f63/3_5.jpg 163w,\n/static/d41d8cd98f00b204e9800998ecf8427e/c989d/3_5.jpg 325w,\n/static/d41d8cd98f00b204e9800998ecf8427e/6aca1/3_5.jpg 650w,\n/static/d41d8cd98f00b204e9800998ecf8427e/7c09c/3_5.jpg 975w,\n/static/d41d8cd98f00b204e9800998ecf8427e/fb1d2/3_5.jpg 1213w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n    </span></p>\n<h3>Perceptron Neuron</h3>\n<p>여러 개의 입력의 weight를 곱해서 Summation을 한다. Activation function의 경우 +1 -1 0 의 표현 방식을 가진 네트워크이다. Weighted Sum이라는 것으로 표현이 되는데 수학적으로 봤을 때 <strong>Linear한 상태이다. 즉, 단순한 모델링밖에 풀 수 없다는 것</strong>이다.</p>\n<p><strong>복잡한 문제를 해결하는 방법은 Layor를 많이 쌓는 것이다.</strong> 만약 Layor가 두 개 이상이 있다고 생각해보자.</p>\n<p>Weighted Sum이 계속 싸이고 그것을 수학적으로 정의한다면 여전히 Linear하게 된다. layor가 많은 Network를 쓴다면 여전히 Linear한 상태가 되고 여전히 효과가 없다는 것이다. 그래서 Nonlinear 방법을 사용해야 한다.</p>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","frontmatter":{"title":"Deep Learning Basic","desc":"Neural Network Perceptron Neuron","thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"fixed","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAE/9oADAMBAAIQAxAAAAHm3C2eoP/EABkQAQACAwAAAAAAAAAAAAAAAAEAAhESIP/aAAgBAQABBQIqs0YmOP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABcQAAMBAAAAAAAAAAAAAAAAAAAgITH/2gAIAQEABj8ChhU//8QAGRABAAIDAAAAAAAAAAAAAAAAAQARIDHh/9oACAEBAAE/IdSuC9RFQpw//9oADAMBAAIAAwAAABDED//EABURAQEAAAAAAAAAAAAAAAAAABAh/9oACAEDAQE/EIf/xAAVEQEBAAAAAAAAAAAAAAAAAAAQIf/aAAgBAgEBPxCn/8QAGxABAAEFAQAAAAAAAAAAAAAAATEAEBEhYdH/2gAIAQEAAT8QDYk7CghNHj2k+ISWxy3/2Q=="},"images":{"fallback":{"src":"/static/c131c100e3af8f5788852f15cd55d03c/56315/deeplearning_basic.jpg","srcSet":"/static/c131c100e3af8f5788852f15cd55d03c/56315/deeplearning_basic.jpg 532w","sizes":"532px"},"sources":[{"srcSet":"/static/c131c100e3af8f5788852f15cd55d03c/1239d/deeplearning_basic.webp 532w","type":"image/webp","sizes":"532px"}]},"width":532,"height":363}}},"date":"2022-01-17","category":"Deep Learning"}}},"pageContext":{"slug":"/blog/deeplearning3/"}},
    "staticQueryHashes": ["1840460387"]}